{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMQA20zhLqFE"
      },
      "source": [
        "DGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zUoUeUqLqkt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from models.mart import MART\n",
        "from types import SimpleNamespace\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UloXQoOTrZqc",
        "outputId": "7ea9cb56-2b93-47df-f7e1-47a43e22e571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 13 10:41:58 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCwSBeGjrZtk",
        "outputId": "059ef9df-4d94-44ec-ca6f-1571c11a66ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.1.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m535.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu118) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu118) (2024.10.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0+cu118)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.0+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.0+cu118) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==2.1.0+cu118 and torchvision==0.15.0+cu118 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch==2.1.0+cu118\n",
            "    torchvision 0.15.0+cu118 depends on torch==2.0.0+cu118\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the correct version of PyTorch with GPU support\n",
        "!pip install torch==2.1.0+cu118 torchvision==0.15.0+cu118 torchaudio==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1gaK_3NrZ1s",
        "outputId": "60bae1e9-feda-4843-9cc2-46287688784c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br_xJHpZQRdK"
      },
      "outputs": [],
      "source": [
        "def load_config(config_path):\n",
        "    \"\"\"\n",
        "    Load configuration from a YAML file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the YAML configuration file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Loaded configuration dictionary.\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] Loading configuration from {config_path}...\")\n",
        "    with open(config_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    print(\"[INFO] Configuration loaded successfully!\")\n",
        "    return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxPMfQnFOZGt"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH = \"files/sdd_ckpt_best.pth\"  # Update this\n",
        "TEST_DATA_PATH = \"files/sdd_test.pkl\"  # Update this\n",
        "CONFIG_PATH = \"files/mart_sdd_reproduce.yaml\"  # Update this\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5qkGgs0OcFi"
      },
      "outputs": [],
      "source": [
        "def load_model(checkpoint_path, opts):\n",
        "    print(\"[INFO] Loading model from checkpoint...\")\n",
        "    args = SimpleNamespace(**opts)\n",
        "    model = MART(args)  # Remove .cuda()\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
        "\n",
        "\n",
        "    model.load_state_dict(checkpoint['state_dict'])  # or whatever loading method you are using\n",
        "\n",
        "    print(\"[INFO] Model loaded successfully!\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA8PRR6LOf6g"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, opts):\n",
        "    print(\"[INFO] Starting evaluation...\")\n",
        "    ade_sum, fde_sum, total_agents = 0.0, 0.0, 0\n",
        "\n",
        "    # Set the device to GPU or CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)  # Move model to device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in test_loader:\n",
        "            x_abs, y = sample\n",
        "            x_abs, y = x_abs.to(device), y.to(device)  # Move tensors to device\n",
        "\n",
        "            batch_size, num_agents, length, _ = x_abs.size()\n",
        "\n",
        "            x_rel = torch.zeros_like(x_abs).to(device)  # Ensure x_rel is on the same device\n",
        "            x_rel[:, :, 1:] = x_abs[:, :, 1:] - x_abs[:, :, :-1]\n",
        "            x_rel[:, :, 0] = x_rel[:, :, 1]\n",
        "\n",
        "            y_pred = model(x_abs, x_rel)\n",
        "\n",
        "            if opts[\"pred_rel\"]:\n",
        "                cur_pos = x_abs[:, :, [-1]].unsqueeze(2)\n",
        "                y_pred = torch.cumsum(y_pred, dim=3) + cur_pos\n",
        "\n",
        "            ade = torch.min(torch.mean(torch.norm(y_pred - y[:, :, None], dim=-1), dim=3), dim=2)[0].mean().item()\n",
        "            fde = torch.min(torch.mean(torch.norm(y_pred[:, :, :, -1:] - y[:, :, None, -1:], dim=-1), dim=3), dim=2)[0].mean().item()\n",
        "\n",
        "            ade_sum += ade * num_agents * batch_size\n",
        "            fde_sum += fde * num_agents * batch_size\n",
        "            total_agents += num_agents * batch_size\n",
        "\n",
        "    ade_avg = (ade_sum / total_agents) * opts[\"scale\"]\n",
        "    fde_avg = (fde_sum / total_agents) * opts[\"scale\"]\n",
        "\n",
        "    print(f\"[INFO] Evaluation Results: ADE = {ade_avg:.4f}, FDE = {fde_avg:.4f}\")\n",
        "    return ade_avg, fde_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFU-woSrfBKJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, obs_len=8, pred_len=12, mode='train', scale=10, inputs=None, max_agents=50\n",
        "    ):\n",
        "        super(TrajectoryDataset, self).__init__()\n",
        "\n",
        "        self.obs_len = obs_len\n",
        "        self.pred_len = pred_len\n",
        "        self.seq_len = self.obs_len + self.pred_len\n",
        "        self.scale = scale\n",
        "        self.max_agents = max_agents\n",
        "\n",
        "        with open('files/sdd_test.pkl'.format(mode), 'rb') as f:\n",
        "            traj = pickle.load(f)\n",
        "\n",
        "        traj_tmp = []\n",
        "\n",
        "        for t in traj:\n",
        "            traj_tmp.append(t)\n",
        "            if mode == 'train':\n",
        "                traj_tmp.append(np.flip(t, axis=1))\n",
        "\n",
        "        self.traj = []\n",
        "        if 'pos_x' in inputs and 'pos_y' in inputs:\n",
        "            for t in traj_tmp:\n",
        "                t -= t[:, :1, :]\n",
        "                self.traj.append(t)\n",
        "        else:\n",
        "            self.traj = traj_tmp\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.traj)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        past_traj = self.traj[index][:, :self.obs_len] * self.scale\n",
        "        future_traj = self.traj[index][:, self.obs_len:] * self.scale\n",
        "\n",
        "        # Padding the number of agents to max_agents if necessary\n",
        "        num_agents = past_traj.shape[0]\n",
        "        if num_agents < self.max_agents:\n",
        "            pad_size = self.max_agents - num_agents\n",
        "            past_traj_padded = np.pad(past_traj, ((0, pad_size), (0, 0), (0, 0)), mode='constant')\n",
        "            future_traj_padded = np.pad(future_traj, ((0, pad_size), (0, 0), (0, 0)), mode='constant')\n",
        "        else:\n",
        "            past_traj_padded = past_traj\n",
        "            future_traj_padded = future_traj\n",
        "\n",
        "        past_traj = torch.from_numpy(past_traj_padded).type(torch.float)\n",
        "        future_traj = torch.from_numpy(future_traj_padded).type(torch.float)\n",
        "\n",
        "        return [past_traj, future_traj]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6LvJHV2Oavo",
        "outputId": "2426bc28-299e-46b6-90fe-fd65a4d24813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading configuration from files/mart_sdd_reproduce.yaml...\n",
            "[INFO] Configuration loaded successfully!\n",
            "[INFO] Loading model from checkpoint...\n",
            "[INFO] PRT Agg: cat\n",
            "[INFO] HRT Agg: avg\n",
            "[INFO] Binary Threshold Function Type: 2\n",
            "[INFO] Model loaded successfully!\n",
            "[INFO] Starting evaluation with test data...\n",
            "[INFO] Starting evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Evaluation Results: ADE = 0.7565, FDE = 1.2026\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "if __name__ == \"__main__\":\n",
        "    opts = load_config(CONFIG_PATH)\n",
        "    model = load_model(CHECKPOINT_PATH, opts)\n",
        "\n",
        "    # Prepare the test dataset and DataLoader\n",
        "    test_dataset = TrajectoryDataset(\n",
        "        obs_len=opts[\"past_length\"],\n",
        "        pred_len=opts[\"future_length\"],\n",
        "        mode=\"test\",  # This loads the test set\n",
        "        scale=opts[\"scale\"],\n",
        "        inputs=opts[\"inputs\"]\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    print(\"[INFO] Starting evaluation with test data...\")\n",
        "    evaluate_model(model, test_loader, opts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('files/sdd_ckpt_best.pth')\n",
        "print(checkpoint.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAJE6-qyvxJ6",
        "outputId": "41bea451-e3f7-47e3-ade8-cde2f3a431cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-834dba6786b4>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('files/sdd_ckpt_best.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['epoch', 'state_dict', 'optimizer'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeWsc5AAtCh0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "f181e0ad-fc4a-49bc-c8f9-0bfffc36edef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: 'Data/biwi_eth_train.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ec3f1d393f4a>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m data_loaders = {\n\u001b[0m\u001b[1;32m     35\u001b[0m     subset: DataLoader(\n\u001b[1;32m     36\u001b[0m         TrajectoryDataset(\n",
            "\u001b[0;32m<ipython-input-25-ec3f1d393f4a>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m data_loaders = {\n\u001b[1;32m     35\u001b[0m     subset: DataLoader(\n\u001b[0;32m---> 36\u001b[0;31m         TrajectoryDataset(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataloader_eth.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, data_dir, obs_len, pred_len, skip, threshold, min_ped, delim, return_index)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mnum_peds_in_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'Data/biwi_eth_train.txt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from copy import deepcopy\n",
        "from collections import OrderedDict\n",
        "import yaml\n",
        "from types import SimpleNamespace\n",
        "from dataloader_eth import TrajectoryDataset, seq_collate  # Imported from your provided dataloader_eth.py\n",
        "\n",
        "# Load model configuration\n",
        "with open('files/mart_sdd_reproduce.yaml', 'r') as file:\n",
        "    config_dict = yaml.safe_load(file)\n",
        "\n",
        "# Convert the config dictionary to an object with attributes\n",
        "config = SimpleNamespace(**config_dict)\n",
        "\n",
        "# Assuming `model` is already loaded in the notebook\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "# Define optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Data preparation using TrajectoryDataset\n",
        "datasets = {\n",
        "    'eth': 'Data/biwi_eth_train.txt',\n",
        "    'hotel': 'Data/biwi_hotel_train.txt',\n",
        "    'univ': 'Data/uni_examples_train.txt',\n",
        "    'zara1': 'Data/crowds_zara01_train.txt',\n",
        "    'zara2': 'Data/crowds_zara02_train.txt',\n",
        "}\n",
        "\n",
        "data_loaders = {\n",
        "    subset: DataLoader(\n",
        "        TrajectoryDataset(\n",
        "            args=config,\n",
        "            data_dir=path,\n",
        "            obs_len=config.past_length,\n",
        "            pred_len=config.future_length,\n",
        "            delim='\\t',\n",
        "        ),\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=seq_collate,\n",
        "    )\n",
        "    for subset, path in datasets.items()\n",
        "}\n",
        "\n",
        "def fine_tune_model(data_loader, model, optimizer, criterion, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in data_loader:\n",
        "            inputs = batch['past_traj'].to(device)\n",
        "            targets = batch['future_traj'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs, None)  # Modify if additional inputs are required\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return deepcopy(model.state_dict())\n",
        "\n",
        "def average_weights(weight_list):\n",
        "    avg_weights = OrderedDict()\n",
        "    for key in weight_list[0].keys():\n",
        "        avg_weights[key] = torch.mean(torch.stack([weights[key] for weights in weight_list]), dim=0)\n",
        "    return avg_weights\n",
        "\n",
        "fine_tuned_weights = []\n",
        "\n",
        "# Fine-tune on each subset\n",
        "for subset_name, data_loader in data_loaders.items():\n",
        "    print(f\"Fine-tuning on {subset_name}...\")\n",
        "    fine_tuned_weights.append(fine_tune_model(data_loader, model, optimizer, criterion))\n",
        "\n",
        "# Perform weight averaging\n",
        "final_weights = average_weights(fine_tuned_weights)\n",
        "\n",
        "# Load averaged weights into the model\n",
        "model.load_state_dict(final_weights)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save({'model_state_dict': model.state_dict()}, 'files/eth_ucy_finetuned.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojqQEJXAuvSl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}